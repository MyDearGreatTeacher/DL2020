#
```
# -*- coding: utf-8 -*-

import tensorflow as tf
c = tf.constant(5)
v = tf.Variable(1)
print(c)
#print(v)
print("{} 階Tensor".format(c.ndim)) 
# scalar(常數就是純量[標量])

x = tf.constant([1, 2, 3, 4, 5, 6]) 
print("{}階Tensor ".format(x.ndim))

x = tf.constant([[1, 2, 3], [4, 5, 6]])
print("{}階Tensor ".format(x.ndim))
```
# Eager Execution 動態圖(TF2)  vs TF1靜態圖
```
https://github.com/taipeitechmmslab/MMSLAB-TF2/blob/master/Lab1.ipynb

https://www.tensorflow.org/api_docs/python/tf/executing_eagerly
```
#

```
s np
import tensorflow as tf

print("Eager Execution 是否啟動: {}".format(tf.executing_eagerly()))
```

# TF 套件架構

### python 模組(module)與套件(package)
```
https://ithelp.ithome.com.tw/articles/10223780
```
### 常用的Tensorflow 2.2模組(module)與套件(package)
```
https://www.tensorflow.org/api_docs/python/tf
https://www.w3cschool.cn/tensorflow_python/tf_io_decode_raw.html


impport tensorflow as tf

tf.io====>檔案存取
tf.data

tf.image
tf.audio

tf.keras===高階開發 HITH-LEVEL
tf.nn===>低階開發 LOWER-LEVEL
```
###
```
tf.io.decode_raw(
    bytes,====>一個string類型的Tensor。所有元素必須具有相同的長度
    out_type,====>tf.DType，可以是：tf.half, tf.float32, tf.float64, tf.int32, 
                   tf.uint16, tf.uint8, tf.int16, tf.int8, tf.int64
    little_endian=True,====>
                            可選的
                            bool。
                            預設為True。
                            輸入bytes是否以little-endian順序排列。
                            忽略存儲在單個位元組（如 uint8）中的out_type值。
                            位元組順序 (Byte Order)，或稱 端序 (Endianness)，
                            即是指 位元組 的排列順序，同理，不同的硬體架構、網路協議… 
                            其用法不盡相同，沒有絕對的好壞，只有適合與否
                            https://blog.csdn.net/hherima/article/details/8639538
    name=None ===>操作的名稱
                  可選的
)


RETURN TYPE=====>一個out_type類型的Tensor
```
### 

```
TensorFlow模組：tf.keras

模組(MODULE)
activations模組： Built-in(內建) activation(激化) functions
applications模組：
backend模組：指定後端平台使用CAFFE 或是TENSORFLOW
callbacks模組：用
constraints模組：
datasets模組：
estimator模組：
initializers模組：
layers模組：用
losses模組：定義一些損失(成本)函數
metrics模組：定義指標
models模組：
optimizers模組：
preprocessing模組：
regularizers模組：
utils模組：
wrappers模組：

類(CLASS)
class Model：Model將圖層分組為具有訓練和推理功能的物件.
class Sequential：線性疊層.

功能
Input(...)：Input()用於產生實體Keras張量.
```
### activations模組 tf.keras.activations
```
https://www.tensorflow.org/api_docs/python/tf/keras/activations
```
```
deserialize(...): Returns activation function denoted by input string.

elu(...): Exponential linear unit.

exponential(...): Exponential activation function.

get(...): Returns function.

hard_sigmoid(...): Hard sigmoid activation function.

linear(...): Linear activation function.

relu(...): Applies the rectified linear unit activation function.

selu(...): Scaled Exponential Linear Unit (SELU).

serialize(...): Returns name attribute (__name__) of function.

sigmoid(...): Sigmoid activation function.

softmax(...): Softmax converts a real vector to a vector of categorical probabilities.

softplus(...): Softplus activation function.

softsign(...): Softsign activation function.

swish(...): Swish activation function.

tanh(...): Hyperbolic tangent activation function.

```
###
```
函數學習重心:
[1]函數的各種參數
[2]函數的回傳結果

最好以案例說明
```
```
tf.keras.activations.relu(
    x, ==================================>Input tensor or variable.
    alpha=0.0, ===========================>A float that governs the slope for values lower than the threshold.
    max_value=None,===========================> A float that sets the saturation threshold 
                                               (the largest value the function will return).
    threshold=0 ===========================> A float giving the threshold value of the activation function below which values 
                                             will be damped or set to zero.
)

Returns函數的回傳結果
A Tensor representing the input tensor, transformed by the relu activation function. 
Tensor will be of the same shape and dtype of input x.
```
### 學生作業
```
tf.keras.activations.softmax(
    x, 
    axis=-1
)
```
# TENSORFLOW 範例 CHAPTER 2 回歸問題
```
"""
### TensorBoard 可視化工具"""

# Commented out IPython magic to ensure Python compatibility.
# 這行指令可以幫助我們直接在jupyter notebook上顯示TensorBoard
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --port 9530 --logdir lab2-logs
```
```
--port 9530    =======>
--logdir lab2-logs =======>設定執行過程的資料存放點
```
##  房價預測之回歸模型
```
Lab2.ipynb

KAGGLE BOSTON HOUSE PREDICTION

https://www.kaggle.com/c/boston-housing
```
```
# -*- coding: utf-8 -*-
import os
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow import keras
from tensorflow.keras import layers

"""### 數據讀取並分析"""

data = pd.read_csv("kc_house_data.csv")
# 顯示dataset的形狀，共21613比資料，每一比資料有21種不同資訊。
data.shape

# 將顯示列數設定為25，不然會有部份資料無法顯示
pd.options.display.max_columns = 25
# head 會顯示前五行的數據
data.head()

"""各個數據的簡寫分別代表下面意思：
- date：房屋出售日期。
- price：房屋價格（目標）。
- bedrooms：臥室數量。
- bathrooms：浴室數量。
- sqft_living：居住的坪數（平方英尺）。
- sqft_lot：實際的坪數（平方英尺）。
- floors：房屋總共樓層。
- waterfront：海景房。
- view：房屋是否看過。
- condition：整體條件有多好。
- grade：房屋的整體等級（根據King County評分系統）。
- sqft_above：除了地下室外的坪數（平方英尺）。
- sqft_basement：地下室的坪數（平方英尺）。
- yr_built：房屋建造時間。
- yr_renovated：何時重新裝修過（一些沒重新裝修過或是裝修紀錄沒被記錄到的數值都為0）。
- zipcode：郵政編碼。
- lat：緯度座標。
- long：經度座標。
- sqft_living15：2015年紀錄的居住坪數（可能是翻新的原因導致sqft_living15與sqft_living不同）。
- sqft_lot15：2015年紀錄的實際坪數（可能是翻新的原因導致sqft_lot15與sqft_lot不同）。

### 檢查資料的型態

資料型態總共有五種：object(string),booleab, integer, float and categorical.
"""

data.dtypes

"""### 數據前處理
轉換資料型態：
因為數據集裡的date數據是字串（string）格式，而模型的輸入只接受數值格式，所以可以透過以下程式碼將其轉為數值，並分成年、月及日三種數據。
"""

# 將date日期拆為年、月和日並轉成數值
data['year'] = pd.to_numeric(data['date'].str.slice(0, 4))
data['month'] = pd.to_numeric(data['date'].str.slice(4, 6))
data['day'] = pd.to_numeric(data['date'].str.slice(6, 8))

# 刪除沒有用的數據，inplace則是將更新後的資料存回原本的地方
data.drop(['id'], axis="columns", inplace=True)
data.drop(['date'], axis="columns", inplace=True)

data.head()

"""分割數據集（Dataset）：將數據集切割成三個部份，訓練數據（Training data）、驗證數據（Validation data）和測試數據（Testing data）。"""

data_num = data.shape[0]
# 取得一筆與data數量相同的亂數索引，主要目的是用於打散資料
indexes = np.random.permutation(data_num)
# 並將亂數索引值分為Train、validation和test分為，這裡的劃分比例為6:2:2
train_indexes = indexes[:int(data_num *0.6)]
val_indexes = indexes[int(data_num *0.6):int(data_num *0.8)]
test_indexes = indexes[int(data_num *0.8):]
# 透過索引值從data取出訓練資料、驗證資料和測試資料
train_data = data.loc[train_indexes]
val_data = data.loc[val_indexes]
test_data = data.loc[test_indexes]

"""### Normalization 正規化

使用標準分數(Standard Score, 又稱z-score)將數據正規化，經過z-score正規化後數據的都會聚集在0附近， 標準差為1。 

(x - 平均值) / 標準差
"""

train_validation_data = pd.concat([train_data, val_data])
mean = train_validation_data.mean()
std = train_validation_data.std()

train_data = (train_data - mean) / std
val_data = (val_data - mean) / std

"""### 建立Numpy array格式的訓練數據"""

x_train = np.array(train_data.drop('price', axis='columns'))
y_train = np.array(train_data['price'])
x_val = np.array(val_data.drop('price', axis='columns'))
y_val = np.array(val_data['price'])

"""整理過後的資料共12967筆，且一筆資料有21種資訊(所以網路輸入必須為21)。"""

x_train.shape

"""### 建立並訓練網路模型

這裡建構三層全連接層的網路架構，並且使用ReLU作為隱藏層的激活函數，而由於需得到線性輸出，故輸出層不使用任何激活函數。
"""

# 建立一個Sequential型態的model
model = keras.Sequential(name='model-1')
# 第1層全連接層設為64個unit，將輸入形狀設定為(21, )，而實際上我們輸入的數據形狀為(batch_size, 21)
model.add(layers.Dense(64, activation='relu', input_shape=(21,)))
# 第2層全連接層設為64個unit
model.add(layers.Dense(64, activation='relu'))
# 最後一層全連接層設為1個unit
model.add(layers.Dense(1))
# 顯示網路模型架構
model.summary()

"""設定訓練使用的優化器、損失函數和指標函數："""

model.compile(keras.optimizers.Adam(0.001),
              loss=keras.losses.MeanSquaredError(),
              metrics=[keras.metrics.MeanAbsoluteError()])

"""創建模型儲存目錄："""

model_dir = 'lab2-logs/models/'
os.makedirs(model_dir)

"""設定回調函數："""

# TensorBoard回調函數會幫忙紀錄訓練資訊，並存成TensorBoard的紀錄檔
log_dir = os.path.join('lab2-logs', 'model-1')
model_cbk = keras.callbacks.TensorBoard(log_dir=log_dir)
# ModelCheckpoint回調函數幫忙儲存網路模型，可以設定只儲存最好的模型，「monitor」表示被監測的數據，「mode」min則代表監測數據越小越好。
model_mckp = keras.callbacks.ModelCheckpoint(model_dir + '/Best-model-1.h5', 
                                        monitor='val_mean_absolute_error', 
                                        save_best_only=True, 
                                        mode='min')

"""訓練網路模型："""

history = model.fit(x_train, y_train,  # 傳入訓練數據
               batch_size=64,  # 批次大小設為64
               epochs=300,  # 整個dataset訓練300遍
               validation_data=(x_val, y_val),  # 驗證數據
               callbacks=[model_cbk, model_mckp])  # Tensorboard回調函數紀錄訓練過程，ModelCheckpoint回調函數儲存最好的模型

"""### 訓練結果"""

history.history.keys()  # 查看history儲存的資訊有哪些

plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='validation')
plt.ylim(0.02, 0.2)
plt.title('Mean square error')
plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend(loc='upper right')

plt.plot(history.history['mean_absolute_error'], label='train')
plt.plot(history.history['val_mean_absolute_error'], label='validation')
plt.ylim(0.12, 0.26)
plt.title('Mean absolute error')
plt.ylabel('metrics')
plt.xlabel('epochs')
plt.legend(loc='upper right')

"""測試數據的誤差百分比：用測試數據預測房屋價格並與答案計算誤差百分比。"""

# 載入模型
model = keras.models.load_model('lab2-logs/models/Best-model-1.h5')
# 先將房屋價格取出
y_test = np.array(test_data['price'])
# 標準化數據
test_data = (test_data - mean) / std
# 將輸入數據存成Numpy 格式
x_test = np.array(test_data.drop('price', axis='columns'))
# 預測測試數據
y_pred = model.predict(x_test)
# 將預測結果轉換回來(因為訓練時的訓練目標也有經過標準化)
y_pred = np.reshape(y_pred * std['price'] + mean['price'], y_test.shape)
# 計算平均的誤差百分比
percentage_error = np.mean(np.abs(y_test - y_pred)) / np.mean(y_test) * 100
# 顯示誤差百分比
print("Model_1 Percentage Error: {:.2f}%".format(percentage_error))
```

# 實驗二：過擬合問題
```
""" 
### 方法一、減少網路權重
"""

model_2 = keras.Sequential(name='model-2')
model_2.add(layers.Dense(16, activation='relu', input_shape=(21,)))
model_2.add(layers.Dense(16, activation='relu'))
model_2.add(layers.Dense(1))

model_2.compile(keras.optimizers.Adam(0.001),
                loss=keras.losses.MeanSquaredError(),
                metrics=[keras.metrics.MeanAbsoluteError()])

log_dir = os.path.join('lab2-logs', 'model-2')
model_cbk = keras.callbacks.TensorBoard(log_dir=log_dir)
model_mckp = keras.callbacks.ModelCheckpoint(model_dir + '/Best-model-2.h5', 
                                             monitor='val_mean_absolute_error', 
                                             save_best_only=True, 
                                             mode='min')
model_2.fit(x_train, y_train, 
            batch_size=64 ,
            epochs=300, 
            validation_data=(x_val, y_val), 
            callbacks=[model_cbk, model_mckp])

"""### 加入L1或L2 正則化"""

model_3 = keras.Sequential(name='model-3')
model_3.add(layers.Dense(64, 
                         kernel_regularizer=keras.regularizers.l2(0.001), 
                         activation='relu', input_shape=(21,)))
model_3.add(layers.Dense(64, kernel_regularizer=keras.regularizers.l2(0.001), activation='relu'))
model_3.add(layers.Dense(1))

model_3.compile(keras.optimizers.Adam(0.001),
                loss=keras.losses.MeanSquaredError(),
                metrics=[keras.metrics.MeanAbsoluteError()])

log_dir = os.path.join('lab2-logs', 'model-3')
model_cbk = keras.callbacks.TensorBoard(log_dir=log_dir)
model_mckp = keras.callbacks.ModelCheckpoint(model_dir + '/Best-model-3.h5', 
                                             monitor='val_mean_absolute_error', 
                                             save_best_only=True, 
                                             mode='min')
model_3.fit(x_train, y_train, 
            batch_size=64 ,
            epochs=300, 
            validation_data=(x_val, y_val), 
            callbacks=[model_cbk, model_mckp])

"""### 加入 Dropout"""

model_4 = keras.Sequential(name='model-4')
model_4.add(layers.Dense(64, activation='relu', input_shape=(21,)))
model_4.add(layers.Dropout(0.3))
model_4.add(layers.Dense(64, activation='relu'))
model_4.add(layers.Dropout(0.3))
model_4.add(layers.Dense(1))

model_4.compile(keras.optimizers.Adam(0.001),
                loss=keras.losses.MeanSquaredError(),
                metrics=[keras.metrics.MeanAbsoluteError()])

log_dir = os.path.join('lab2-logs', 'model-4')
model_cbk = keras.callbacks.TensorBoard(log_dir=log_dir)
model_mckp = keras.callbacks.ModelCheckpoint(model_dir + '/Best-model-4.h5', 
                                             monitor='val_mean_absolute_error', 
                                             save_best_only=True, 
                                             mode='min')
model_4.fit(x_train, y_train, 
            batch_size=64 ,
            epochs=300, 
            validation_data=(x_val, y_val), 
            callbacks=[model_cbk, model_mckp])

"""
### 驗證正則化的效能
Test model 2:
"""
model_2 = keras.models.load_model('lab2-logs/models/Best-model-2.h5')
y_pred = model_2.predict(x_test)
y_pred = np.reshape(y_pred * std['price'] + mean['price'], y_test.shape)
percentage_error = np.mean(np.abs(y_test - y_pred)) / np.mean(y_test) * 100
print("Model_2: {:.2f}%".format(percentage_error))

"""Test model 3:"""
model_3 = keras.models.load_model('lab2-logs/models/Best-model-3.h5')
y_pred = model_3.predict(x_test)
y_pred = np.reshape(y_pred * std['price'] + mean['price'], y_test.shape)
percentage_error = np.mean(np.abs(y_test - y_pred)) / np.mean(y_test) * 100
print("Model_3: {:.2f}%".format(percentage_error))

"""Test model 4:"""
model_4 = keras.models.load_model('lab2-logs/models/Best-model-4.h5')
y_pred = model_4.predict(x_test)
y_pred = np.reshape(y_pred * std['price'] + mean['price'], y_test.shape)
percentage_error = np.mean(np.abs(y_test - y_pred)) / np.mean(y_test) * 100
print("Model_4: {:.2f}%".format(percentage_error))
```
