#
```
# -*- coding: utf-8 -*-

import tensorflow as tf
c = tf.constant(5)
v = tf.Variable(1)
print(c)
#print(v)
print("{} 階Tensor".format(c.ndim)) 
# scalar(常數就是純量[標量])

x = tf.constant([1, 2, 3, 4, 5, 6]) 
print("{}階Tensor ".format(x.ndim))

x = tf.constant([[1, 2, 3], [4, 5, 6]])
print("{}階Tensor ".format(x.ndim))
```
# Eager Execution 動態圖(TF2)  vs TF1靜態圖
```
https://github.com/taipeitechmmslab/MMSLAB-TF2/blob/master/Lab1.ipynb

https://www.tensorflow.org/api_docs/python/tf/executing_eagerly
```
#

```
s np
import tensorflow as tf

print("Eager Execution 是否啟動: {}".format(tf.executing_eagerly()))
```

# TF 套件架構

### python 模組(module)與套件(package)
```
https://ithelp.ithome.com.tw/articles/10223780
```
### 常用的Tensorflow 2.2模組(module)與套件(package)
```
https://www.tensorflow.org/api_docs/python/tf
https://www.w3cschool.cn/tensorflow_python/tf_io_decode_raw.html


impport tensorflow as tf

tf.io====>檔案存取
tf.data

tf.image
tf.audio

tf.keras===高階開發 HITH-LEVEL
tf.nn===>低階開發 LOWER-LEVEL
```
###
```
tf.io.decode_raw(
    bytes,====>一個string類型的Tensor。所有元素必須具有相同的長度
    out_type,====>tf.DType，可以是：tf.half, tf.float32, tf.float64, tf.int32, 
                   tf.uint16, tf.uint8, tf.int16, tf.int8, tf.int64
    little_endian=True,====>
                            可選的
                            bool。
                            預設為True。
                            輸入bytes是否以little-endian順序排列。
                            忽略存儲在單個位元組（如 uint8）中的out_type值。
                            位元組順序 (Byte Order)，或稱 端序 (Endianness)，
                            即是指 位元組 的排列順序，同理，不同的硬體架構、網路協議… 
                            其用法不盡相同，沒有絕對的好壞，只有適合與否
                            https://blog.csdn.net/hherima/article/details/8639538
    name=None ===>操作的名稱
                  可選的
)


RETURN TYPE=====>一個out_type類型的Tensor
```
### 

```
TensorFlow模組：tf.keras

模組(MODULE)
activations模組： Built-in(內建) activation(激化) functions
applications模組：
backend模組：指定後端平台使用CAFFE 或是TENSORFLOW
callbacks模組：用
constraints模組：
datasets模組：
estimator模組：
initializers模組：
layers模組：用
losses模組：定義一些損失(成本)函數
metrics模組：定義指標
models模組：
optimizers模組：
preprocessing模組：
regularizers模組：
utils模組：
wrappers模組：

類(CLASS)
class Model：Model將圖層分組為具有訓練和推理功能的物件.
class Sequential：線性疊層.

功能
Input(...)：Input()用於產生實體Keras張量.
```
### activations模組 tf.keras.activations
```
https://www.tensorflow.org/api_docs/python/tf/keras/activations
```
```
deserialize(...): Returns activation function denoted by input string.

elu(...): Exponential linear unit.

exponential(...): Exponential activation function.

get(...): Returns function.

hard_sigmoid(...): Hard sigmoid activation function.

linear(...): Linear activation function.

relu(...): Applies the rectified linear unit activation function.

selu(...): Scaled Exponential Linear Unit (SELU).

serialize(...): Returns name attribute (__name__) of function.

sigmoid(...): Sigmoid activation function.

softmax(...): Softmax converts a real vector to a vector of categorical probabilities.

softplus(...): Softplus activation function.

softsign(...): Softsign activation function.

swish(...): Swish activation function.

tanh(...): Hyperbolic tangent activation function.

```
###
```
函數學習重心:
[1]函數的各種參數
[2]函數的回傳結果

最好以案例說明
```
```
tf.keras.activations.relu(
    x, ==================================>Input tensor or variable.
    alpha=0.0, ===========================>A float that governs the slope for values lower than the threshold.
    max_value=None,===========================> A float that sets the saturation threshold 
                                               (the largest value the function will return).
    threshold=0 ===========================> A float giving the threshold value of the activation function below which values 
                                             will be damped or set to zero.
)

Returns函數的回傳結果
A Tensor representing the input tensor, transformed by the relu activation function. 
Tensor will be of the same shape and dtype of input x.
```
### 學生作業
```
tf.keras.activations.softmax(
    x, 
    axis=-1
)
```
